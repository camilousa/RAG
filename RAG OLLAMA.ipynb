{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42f1952-8bdf-42d4-b40f-ed1f703ddeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "%pip install -qU langchain-ollama langchain-postgres\n",
    "%pip install -qU boto3 pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c869503e-2125-4845-a9a4-343cdd78482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import getpass\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "import requests\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea2b8b-3ea7-479f-bc4f-3dfdbc6d27c1",
   "metadata": {},
   "source": [
    "# Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d19f16-c2b8-40c7-ae3d-928a3e287eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LangSmith API Key:  ········\n",
      "PostgreSQL URI (postgresql+psycopg://...):  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")\n",
    "POSTGRES_URI = getpass.getpass(\"PostgreSQL URI (postgresql+psycopg://...): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a988863-16ba-4903-86b7-a8112ecfd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNgrokChat(BaseChatModel):\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom-ngrok-chat\"\n",
    "\n",
    "    def _generate(self, messages, stop=None, **kwargs) -> ChatResult:\n",
    "\n",
    "        prompt = \"\\n\".join([m.content for m in messages if isinstance(m, HumanMessage)])\n",
    "\n",
    "        response = requests.post(\n",
    "            \"https://2c96-34-19-62-22.ngrok-free.app/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3.2\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        answer = response.json()[\"response\"]\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=answer))])\n",
    "\n",
    "llm = CustomNgrokChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d49ce1-31f5-4804-9813-4976bd4d6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteOllamaEmbeddings(Embeddings):\n",
    "    def __init__(self, endpoint: str, model: str = \"nomic-embed-text\"):\n",
    "        self.endpoint = endpoint\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            res = requests.post(\n",
    "                f\"{self.endpoint}/api/embeddings\",\n",
    "                json={\"model\": self.model, \"prompt\": text}\n",
    "            )\n",
    "            res.raise_for_status()\n",
    "            results.append(res.json()[\"embedding\"])\n",
    "        return results\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "embeddings = RemoteOllamaEmbeddings(endpoint=\"http://52.90.98.121:11434\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90f6082-01cf-49c5-a725-e8ee34bfcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"pdf_docs\",\n",
    "    connection=POSTGRES_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9744f-ea6c-41bf-837a-c48cfc673713",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe1874f7-1ca7-4b62-937c-a7de70b9835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 56 documentos PDF desde S3.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"docsragusa\"\n",
    "prefix = \"\" \n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    key = obj[\"Key\"]\n",
    "    if key.endswith(\".pdf\"):\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False) as tmp:\n",
    "            s3.download_fileobj(bucket_name, key, tmp)\n",
    "            tmp.flush()\n",
    "            loader = PyPDFLoader(tmp.name)\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "print(f\"Cargados {len(all_docs)} documentos PDF desde S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a80e98-ed09-4fc4-a8bd-02688832027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f782fe95-020d-4858-8d60-9bdf1b9d5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e178f-f6d9-493c-b96e-97bf317b71f9",
   "metadata": {},
   "source": [
    "# Recuperación de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39e35f8-c7ff-44d7-bc68-cc00b7ec7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    print(state[\"question\"])\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    print(retrieved_docs)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    chat_prompt_value = prompt.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": docs_content,\n",
    "        \"instruction\": \"Answer clearly and concisely.\"\n",
    "    })\n",
    "\n",
    "    # Usa directamente la lista de mensajes para la llamada al LLM\n",
    "    response = llm.invoke(chat_prompt_value.messages)\n",
    "\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b8827c-0794-443d-b81c-2b19d010cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corregir_codificacion(texto):\n",
    "    try:\n",
    "        # Intenta re-interpretar la cadena\n",
    "        return texto.encode('latin1').decode('utf8')\n",
    "    except:\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d1cf7-2865-4a06-bf29-4860c97afc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_question = input(\"💗 Por favor escribe tu pregunta (o 'salir' para terminar): \")\n",
    "    if user_question.lower() in (\"salir\", \"exit\", \"quit\"):\n",
    "        print(\"👋 ¡Gracias por usar el chat! Hasta luego.\")\n",
    "        break\n",
    "    response = graph.invoke({\"question\": user_question})\n",
    "    answer = response[\"answer\"]\n",
    "    answer_corregida = corregir_codificacion(answer)\n",
    "    print(\"\\nBOT🤖: \")\n",
    "    print(answer_corregida)\n",
    "    print(\"\\n\" + \"-\"*100 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
